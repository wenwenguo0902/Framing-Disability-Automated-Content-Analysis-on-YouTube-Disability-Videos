{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612bd424",
   "metadata": {},
   "source": [
    "# YouTube API Data Retrieval\n",
    "NOTE: Since it takes time and resources to retrieve the YouTube data, I will not re-run the lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae8f9a",
   "metadata": {},
   "source": [
    "## 0. Preparation: Install Necessary Python Packages （google-api-python-client and youtube-transcript-api）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767a1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-api-python-client youtube-transcript-api\n",
    "# !pip install lxml requests\n",
    "# ! pip install -r requirements.txt\n",
    "# ! pip install --upgrade google-api-python-client\n",
    "# ! pip install textblob\n",
    "# ! pip install selenium\n",
    "#! pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0865ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import requests\n",
    "import re\n",
    "from lxml import html\n",
    "from googleapiclient.discovery import build\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58c250",
   "metadata": {},
   "source": [
    "## 1. Retrieve Youtube Video IDs\n",
    "This is fulfilled using the Youtube API v3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889c6a0e",
   "metadata": {},
   "source": [
    "Initialize the api_key. This is activated from the google cloud console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfcdf818",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyBBtbun3W1dFNNJd9UT5E4cIzlBqnfR2hQ'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b915b",
   "metadata": {},
   "source": [
    "Define a function that retrieves the relevant Youtube video IDs.\n",
    "\n",
    "Due to request limitations, the requests are retrieved year-by-year, so that the queries are not exhausted. If the query is used up, a back-up API is used to continue with data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6258fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_youtube_video_ids(api_key, published_after, published_before, max_results=1500):\n",
    "    # Build the YouTube client\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    \n",
    "    # List to store video IDs\n",
    "    video_ids = []\n",
    "    \n",
    "    # Initial search parameters\n",
    "    request = youtube.search().list(\n",
    "        part=\"id\",\n",
    "        type=\"video\",\n",
    "        maxResults=50,  # Maximum allowed per API documentation\n",
    "        q=\"disability\",  # Search query\n",
    "        publishedAfter=published_after,\n",
    "        publishedBefore=published_before\n",
    "    )\n",
    "    \n",
    "    while request and len(video_ids) < max_results:\n",
    "        # Execute the API request\n",
    "        response = request.execute()\n",
    "        \n",
    "        # Extract video IDs from the response\n",
    "        video_ids.extend([item['id']['videoId'] for item in response.get('items', [])])\n",
    "        \n",
    "        if 'nextPageToken' in response:\n",
    "            request = youtube.search().list_next(request, response)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        # Pause to avoid quota errors\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return video_ids[:max_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa5e62",
   "metadata": {},
   "source": [
    "Run the function to retrieve relevant videos, and save them separately in json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "491a7dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a save json function for easier use later.\n",
    "def save_json(file_name, data):\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43219e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 514 video IDs in 2016.\n"
     ]
    }
   ],
   "source": [
    "video_ids16 = retrieve_youtube_video_ids(api_key, '2016-01-01T00:00:00Z', '2016-12-31T23:59:59Z')\n",
    "print(f\"Retrieved {len(video_ids16)} video IDs in 2016.\")\n",
    "\n",
    "save_json('Disability_video_ids16.json', video_ids16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa66519c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 560 video IDs in 2017.\n"
     ]
    }
   ],
   "source": [
    "video_ids17 = retrieve_youtube_video_ids(api_key, '2017-01-01T00:00:00Z', '2017-12-31T23:59:59Z')\n",
    "print(f\"Retrieved {len(video_ids17)} video IDs in 2017.\")\n",
    "\n",
    "save_json('Disability_video_ids17.json', video_ids17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d98a02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 554 video IDs in 2018.\n"
     ]
    }
   ],
   "source": [
    "video_ids18 = retrieve_youtube_video_ids(api_key, '2018-01-01T00:00:00Z', '2018-12-31T23:59:59Z')\n",
    "print(f\"Retrieved {len(video_ids18)} video IDs in 2018.\")\n",
    "\n",
    "save_json('Disability_video_ids18.json', video_ids18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "373a804a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 550 video IDs in 2019.\n"
     ]
    }
   ],
   "source": [
    "video_ids19 = retrieve_youtube_video_ids(api_key, '2019-01-01T00:00:00Z', '2019-12-31T23:59:59Z')\n",
    "print(f\"Retrieved {len(video_ids19)} video IDs in 2019.\")\n",
    "\n",
    "save_json('Disability_video_ids19.json', video_ids19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "614827d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 609 video IDs in 2020.\n"
     ]
    }
   ],
   "source": [
    "video_ids20 = retrieve_youtube_video_ids(api_key, '2020-01-01T00:00:00Z', '2020-12-31T23:59:59Z')\n",
    "print(f\"Retrieved {len(video_ids20)} video IDs in 2020.\")\n",
    "\n",
    "save_json('Disability_video_ids20.json', video_ids20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7793f450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 624 video IDs in 2021.\n"
     ]
    }
   ],
   "source": [
    "video_ids21 = retrieve_youtube_video_ids(api_key, '2021-01-01T00:00:00Z', '2021-12-31T23:59:59Z')\n",
    "print(f\"Retrieved {len(video_ids21)} video IDs in 2021.\")\n",
    "\n",
    "save_json('Disability_video_ids21.json', video_ids21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d80612ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 579 video IDs in 2022.\n"
     ]
    }
   ],
   "source": [
    "video_ids22 = retrieve_youtube_video_ids(api_key, '2022-01-01T00:00:00Z', '2022-12-31T23:59:59Z')\n",
    "print(f\"Retrieved {len(video_ids22)} video IDs in 2022.\")\n",
    "\n",
    "save_json('Disability_video_ids22.json', video_ids22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad74a569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 602 video IDs in 2023.\n"
     ]
    }
   ],
   "source": [
    "video_ids23 = retrieve_youtube_video_ids(api_key, '2023-01-01T00:00:00Z', '2023-12-31T23:59:59Z')\n",
    "print(f\"Retrieved {len(video_ids23)} video IDs in 2023.\")\n",
    "\n",
    "save_json('Disability_video_ids23.json', video_ids23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66b5514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 546 video IDs in 2024.\n"
     ]
    }
   ],
   "source": [
    "video_ids24 = retrieve_youtube_video_ids(api_key, '2024-01-01T00:00:00Z', '2024-12-31T23:59:59Z')\n",
    "print(f\"Retrieved {len(video_ids24)} video IDs in 2024.\")\n",
    "\n",
    "save_json('Disability_video_ids24.json', video_ids24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea7e5d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been merged successfully.\n"
     ]
    }
   ],
   "source": [
    "# List of file names to merge\n",
    "file_names = [\"Disability_video_ids16.json\", \n",
    "              \"Disability_video_ids17.json\", \n",
    "              \"Disability_video_ids18.json\", \n",
    "              \"Disability_video_ids19.json\", \n",
    "              \"Disability_video_ids20.json\", \n",
    "              \"Disability_video_ids21.json\", \n",
    "              \"Disability_video_ids22.json\", \n",
    "              \"Disability_video_ids23.json\", \n",
    "              \"Disability_video_ids24.json\"]\n",
    "\n",
    "# Initialize a list to hold all combined data\n",
    "merged_files = []\n",
    "\n",
    "# Loop through each file and read the data\n",
    "for file in file_names:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        merged_files.extend(data)  # Assuming data is a list and can be extended\n",
    "\n",
    "# Write the combined data to a new file\n",
    "with open('Disability_video_ids_16-24.json', 'w') as f:\n",
    "    json.dump(merged_files, f)\n",
    "\n",
    "print(\"Files have been merged successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e6b1a",
   "metadata": {},
   "source": [
    "## 2. Retrieve relevant parameters for future analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1aed16",
   "metadata": {},
   "source": [
    "Using the Google API, the following elements are retrieved:\n",
    "1. the title of the video;\n",
    "2. the description of the video;\n",
    "3. the published year of the video;\n",
    "4. the channel of the video.\n",
    "These are all publically available through the Google official API, which can be seen in the documentation (https://developers.google.com/youtube/v3/docs/?apix=true)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c04d30",
   "metadata": {},
   "source": [
    "Load the video ids retrieved in the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4ba5c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch and save video details to a CSV file\n",
    "def fetch_and_save_video_details(input_json, output_csv, completion_message):\n",
    "    # Load video IDs from JSON file\n",
    "    with open(input_json, 'r') as f:\n",
    "        disability_video_ids = json.load(f)\n",
    "\n",
    "    # List to store details of all videos\n",
    "    video_details = []\n",
    "    \n",
    "    # Loop through all video IDs to get their details\n",
    "    for video_id in disability_video_ids:\n",
    "        # Make an API call to get the video details\n",
    "        youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "        response = youtube.videos().list(\n",
    "            part='snippet',\n",
    "            id=video_id\n",
    "        ).execute()\n",
    "\n",
    "        # Check if the response contains any items\n",
    "        if 'items' in response and response['items']:\n",
    "            item = response['items'][0]\n",
    "            title = item['snippet']['title']\n",
    "            description = item['snippet']['description']\n",
    "            publish_year = item['snippet']['publishedAt'][:4]  # Taking the first four characters gives us the year\n",
    "            channel = item['snippet']['channelTitle']\n",
    "\n",
    "            # Append details including video ID\n",
    "            video_details.append({\n",
    "                'Video ID': video_id,\n",
    "                'Title': title,\n",
    "                'Published Year': publish_year,\n",
    "                'Description': description,\n",
    "                'Channel': channel\n",
    "            })\n",
    "        else:\n",
    "            video_details.append({\n",
    "                'Video ID': video_id,\n",
    "                'Error': 'Video not found'\n",
    "            })\n",
    "\n",
    "    # Saving the details to a CSV file\n",
    "    with open(output_csv, 'w', newline='') as f:\n",
    "        fieldnames = ['Video ID', 'Title', 'Published Year', 'Description', 'Channel']\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(video_details)\n",
    "\n",
    "    # Print completion message with the output file name included\n",
    "    print(completion_message.format(output_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea932a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyBBtbun3W1dFNNJd9UT5E4cIzlBqnfR2hQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0baa58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# back-up API key\n",
    "# api_key = 'AIzaSyDXvl5gVJP5Kqy2m8rVaVXTihGjrnD_MPA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "372c4909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video details for 2016 fetched and saved successfully in Disability_videos_16.csv.\n"
     ]
    }
   ],
   "source": [
    "fetch_and_save_video_details('Disability_video_ids16.json', 'Disability_videos_16.csv', \"Video details for 2016 fetched and saved successfully in {}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8e73481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video details for 2017 fetched and saved successfully in Disability_videos_17.csv.\n"
     ]
    }
   ],
   "source": [
    "fetch_and_save_video_details('Disability_video_ids17.json', 'Disability_videos_17.csv', \"Video details for 2017 fetched and saved successfully in {}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46e6642e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video details for 2018 fetched and saved successfully in Disability_videos_18.csv.\n"
     ]
    }
   ],
   "source": [
    "fetch_and_save_video_details('Disability_video_ids18.json', 'Disability_videos_18.csv', \"Video details for 2018 fetched and saved successfully in {}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95cc570f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video details for 2019 fetched and saved successfully in Disability_videos_19.csv.\n"
     ]
    }
   ],
   "source": [
    "fetch_and_save_video_details('Disability_video_ids19.json', 'Disability_videos_19.csv', \"Video details for 2019 fetched and saved successfully in {}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97e82cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video details for 2020 fetched and saved successfully in Disability_videos_20.csv.\n"
     ]
    }
   ],
   "source": [
    "fetch_and_save_video_details('Disability_video_ids20.json', 'Disability_videos_20.csv', \"Video details for 2020 fetched and saved successfully in {}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a70a31ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video details for 2021 fetched and saved successfully in Disability_videos_21.csv.\n"
     ]
    }
   ],
   "source": [
    "fetch_and_save_video_details('Disability_video_ids21.json', 'Disability_videos_21.csv', \"Video details for 2021 fetched and saved successfully in {}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2bd9823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video details for 2022 fetched and saved successfully in Disability_videos_22.csv.\n"
     ]
    }
   ],
   "source": [
    "fetch_and_save_video_details('Disability_video_ids22.json', 'Disability_videos_22.csv', \"Video details for 2022 fetched and saved successfully in {}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a367e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video details for 2023 fetched and saved successfully in Disability_videos_23.csv.\n"
     ]
    }
   ],
   "source": [
    "fetch_and_save_video_details('Disability_video_ids23.json', 'Disability_videos_23.csv', \"Video details for 2023 fetched and saved successfully in {}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ee21ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video details for 2024 fetched and saved successfully in Disability_videos_24.csv.\n"
     ]
    }
   ],
   "source": [
    "fetch_and_save_video_details('Disability_video_ids24.json', 'Disability_videos_24.csv', \"Video details for 2024 fetched and saved successfully in {}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc58ce",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7754bfef",
   "metadata": {},
   "source": [
    "Combine all csvs to form the final video details csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7f8377d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been combined into Disability_videos_combined.csv.\n"
     ]
    }
   ],
   "source": [
    "file_names = [\n",
    "    'Disability_videos_16.csv',\n",
    "    'Disability_videos_17.csv',\n",
    "    'Disability_videos_18.csv',\n",
    "    'Disability_videos_19.csv',\n",
    "    'Disability_videos_20.csv',\n",
    "    'Disability_videos_21.csv',\n",
    "    'Disability_videos_22.csv',\n",
    "    'Disability_videos_23.csv',\n",
    "    'Disability_videos_24.csv'\n",
    "]\n",
    "\n",
    "# Use pandas to read and concatenate all files into one DataFrame\n",
    "combined_df = pd.concat([pd.read_csv(f) for f in file_names], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('Disability_videos_combined.csv', index=False)\n",
    "\n",
    "print(\"All files have been combined into Disability_videos_combined.csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa5cf29",
   "metadata": {},
   "source": [
    "the total sample consists of 5138 videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e70ae",
   "metadata": {},
   "source": [
    "Shuffle the rows for labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67d291c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('Disability_videos_combined.csv')\n",
    "\n",
    "# Shuffle the rows of the DataFrame\n",
    "shuffled_df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Save the shuffled DataFrame back to CSV\n",
    "shuffled_df.to_csv('Disability_videos_combined_shuffled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba47e1f2",
   "metadata": {},
   "source": [
    "Drop Nas and non-english Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71272d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df = shuffled_df.dropna(subset=['Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8c83d",
   "metadata": {},
   "source": [
    "4636 videos are left after dropping the NA videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7107265a",
   "metadata": {},
   "source": [
    "Drop the non-english descriptions:\n",
    "Here, the langdetect library is used to detect languages. https://pypi.org/project/langdetect/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4c152dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "# Filter for English descriptions\n",
    "shuffled_df['is_english'] = shuffled_df['Description'].apply(is_english)\n",
    "shuffled_df = shuffled_df[shuffled_df['is_english']]\n",
    "shuffled_df.drop('is_english', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fb09fa",
   "metadata": {},
   "source": [
    "4349 videos are left after dropping the non-English descriptions.\n",
    "\n",
    "Save the final video details csv into Disability_videos_combined_shuffled_cleaned.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c2a0eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data\n",
    "shuffled_df.to_csv('Disability_videos_combined_shuffled_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca20ba5",
   "metadata": {},
   "source": [
    "Split the csv file into two parts, one for labeling and training, one for future classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "680e4e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file split into Disability_videos_combined_shuffled_cleaned_labeled.csv and Disability_videos_combined_shuffled_cleaned_unlabeled.csv.\n"
     ]
    }
   ],
   "source": [
    "shuffled_df_labeled = shuffled_df.iloc[:251]\n",
    "shuffled_df_unlabeled = shuffled_df.iloc[251:]\n",
    "\n",
    "shuffled_df_labeled.to_csv('Disability_videos_combined_shuffled_cleaned_labeled.csv', index=False)\n",
    "shuffled_df_unlabeled.to_csv('Disability_videos_combined_shuffled_cleaned_unlabeled.csv', index=False)\n",
    "\n",
    "print(f\"CSV file split into Disability_videos_combined_shuffled_cleaned_labeled.csv and Disability_videos_combined_shuffled_cleaned_unlabeled.csv.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
